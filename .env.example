# Screen Vision - Local Ollama Edition Configuration
# Copy this file to .env and customize as needed

# =============================================================================
# LOCAL OLLAMA CONFIGURATION (DEFAULT)
# =============================================================================

# Ollama server URL (default: http://localhost:11434)
OLLAMA_BASE_URL=http://localhost:11434

# Default vision model to use with Ollama
# Options: qwen3-vl, llava, bakllava, llava-phi3, etc.
OLLAMA_MODEL=qwen3-vl

# =============================================================================
# CLOUD PROVIDER CONFIGURATION (OPTIONAL)
# =============================================================================

# OpenRouter API Key for cloud models (optional)
# Get your key from: https://openrouter.ai/keys
# OPENROUTER_API_KEY=sk-or-v1-...

# OpenRouter model to use (if OPENROUTER_API_KEY is set)
# Options: qwen/qwen3-vl-30b-a3b-instruct, meta-llama/llama-3.2-90b-vision-instruct, etc.
# OPENROUTER_MODEL=qwen/qwen3-vl-30b-a3b-instruct

# =============================================================================
# CUSTOM PROVIDER CONFIGURATION (OPTIONAL)
# =============================================================================

# Custom API base URL for other local providers
# CUSTOM_API_BASE_URL=http://localhost:8080

# Custom API key for custom provider
# CUSTOM_API_KEY=your-key

# Custom model name for custom provider
# CUSTOM_MODEL=your-model

# =============================================================================
# ADVANCED CONFIGURATION
# =============================================================================

# Server configuration
HOST=0.0.0.0
PORT=8000

# Enable/disable debug mode
DEBUG=false

# Maximum request size (in bytes)
MAX_REQUEST_SIZE=10485760

# Rate limiting (requests per minute)
RATE_LIMIT=20

# =============================================================================
# GPU CONFIGURATION
# =============================================================================

# Force GPU usage (ollama specific)
# Options: nvidia, rocm, cpu
# OLLAMA_GPU=nvidia

# Number of GPU layers to use (0 = all)
# OLLAMA_GPU_LAYERS=0

# =============================================================================
# EXAMPLE CONFIGURATIONS
# =============================================================================

# Example 1: Use Ollama with different model
# OLLAMA_MODEL=llava

# Example 2: Use OpenRouter cloud models
# OPENROUTER_API_KEY=sk-or-v1-your-key-here
# OPENROUTER_MODEL=meta-llama/llama-3.2-90b-vision-instruct

# Example 3: Use custom local provider
# CUSTOM_API_BASE_URL=http://localhost:8080
# CUSTOM_API_KEY=your-custom-key
# CUSTOM_MODEL=your-custom-model

# Example 4: Development configuration
# DEBUG=true
# HOST=127.0.0.1
# PORT=8001
